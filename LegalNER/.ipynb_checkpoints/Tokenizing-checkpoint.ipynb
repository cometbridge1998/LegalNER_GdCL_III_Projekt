{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550855c0-9cf8-44a0-a9e0-a3a3eb930a16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tokenizing\n",
    "<br>\n",
    "Purpose of this notebook is to convert the annotated judgement texts from the <b> javascript objects (json) </b> into <b> pandas dataframes </b>, which can be used as the matrix for mashine learning.<br> <br>\n",
    "Labels in the annotated texts are stored in the json trees according to the sequence number of characters. <br>\n",
    "<i> e.g. \" ... Hongkong Bank ... \" - { 'value': {'start': 90, 'end': 103, 'text': 'Hongkong Bank','labels': [\"ORG\"] } </i> <br> <br> \n",
    "With the <b> span_tokenize </b> the labels will be adapted to the sequence of tokens. <br>\n",
    "<i> e.g. [ ... 'B-ORG', 'I-ORG', ... ] </i> <br> <br> \n",
    "Each token and its label makes up a single row in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b4dda-35d4-4ae9-8300-744dcd50a067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff22a8b-6539-4a98-8ab9-d71ca63c27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3392eb0e-05dd-4c35-a110-f5621d15921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b64963-8174-4209-83a6-5dc4462dad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce2d8d5-7809-4c3b-aed3-3b904dd4ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training dataset\n",
    "with open(\"NER_TRAIN/NER_TRAIN_JUDGEMENT.json\") as json_file_train:\n",
    "    json_object_train = json.load(json_file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6173783-8adf-430c-a3dd-955f44698e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the developing dataset\n",
    "with open(\"NER_DEV/NER_DEV/NER_DEV_JUDGEMENT.json\") as json_file_dev:\n",
    "    json_object_dev = json.load(json_file_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9aa8dd-446f-40ae-8d31-6dbed23e424e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caee6f4d-4bf1-41c2-b6a0-e86afa438f20",
   "metadata": {},
   "source": [
    "<br>\n",
    "Have a look at th first sentence of the judgment. (the first json tree) <br>\n",
    "Each tree includes only one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c36ac1-0e44-4536-9b55-1a53485b7fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '90d9a97c7b7749ec8a4f460fda6f937e',\n",
       " 'annotations': [{'result': [{'value': {'start': 90,\n",
       "      'end': 103,\n",
       "      'text': 'Hongkong Bank',\n",
       "      'labels': ['ORG']},\n",
       "     'id': 'C8HPTIM1',\n",
       "     'from_name': 'label',\n",
       "     'to_name': 'text',\n",
       "     'type': 'labels'},\n",
       "    {'value': {'start': 267,\n",
       "      'end': 278,\n",
       "      'text': 'Rahul & Co.',\n",
       "      'labels': ['ORG']},\n",
       "     'id': 'KOWE3RAM',\n",
       "     'from_name': 'label',\n",
       "     'to_name': 'text',\n",
       "     'type': 'labels'}]}],\n",
       " 'data': {'text': \"\\n\\n(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\"},\n",
       " 'meta': {'source': 'tax_districtcourts judgement https://indiankanoon.org/doc/1556717/'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_object_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a609b-52f0-4fc2-b72b-747ab6220a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fbcc709-cc97-4c28-8f41-1d2b57272b49",
   "metadata": {},
   "source": [
    "### get_start_and_end_and_labels\n",
    "This function is designed to extract all labels with their spans from a json tree. <br>\n",
    "It returns a list of labels. (length of the list corresponds to the number of total labels in the sentence. A label that spans more than one tokens count also as only one label) <br>\n",
    "Each label is list of three elements.<br>\n",
    "The first element is the name of the label, e. g. \"ORG\". <br>\n",
    "The second element is the start of label, the third the end of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2bb06e5-c7cf-4cfd-ab27-629df0fa81c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_and_end_and_labels(tree):\n",
    "    start_and_end_and_labels = []\n",
    "    for label in tree[\"annotations\"][0][\"result\"]:\n",
    "        labels = label[\"value\"][\"labels\"][0]\n",
    "        start = label[\"value\"][\"start\"]\n",
    "        end = label[\"value\"][\"end\"]\n",
    "        start_and_end_and_labels.append([labels, start, end])\n",
    "    return start_and_end_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eea3d1-b2d9-492f-a8a5-6959080ffd9c",
   "metadata": {},
   "source": [
    "<br>\n",
    "The labels in the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0f8b16-c75a-4871-a95d-c6f8fc62d9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ORG', 90, 103], ['ORG', 267, 278]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_start_and_end_and_labels(json_object_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff378c31-d3e0-4fe2-9ac7-6ae452d279b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb2016d-857a-44e2-9d79-2ab8c219a19c",
   "metadata": {},
   "source": [
    "### return_text_and_label\n",
    "returns a tuple in length of two. <br>\n",
    "The first element is the text of the tree. (str) <br>\n",
    "The second a list of all labels. (list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fada8060-556c-4941-91f0-5e08b09a4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_text_and_label(tree):\n",
    "    text = tree[\"data\"][\"text\"]\n",
    "    labels = get_start_and_end_and_labels(tree)\n",
    "    return text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b98f2b90-eecf-4d64-8aba-39420b82a49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"\\n\\n(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\", [['ORG', 90, 103], ['ORG', 267, 278]])\n"
     ]
    }
   ],
   "source": [
    "print(return_text_and_label(json_object_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a2bda-3197-4a29-ad3c-c0f6174d2c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "085d6258-f9bc-499f-84f7-5e41ec2714a1",
   "metadata": {},
   "source": [
    "### Have a try of the TreebankWordTokenizer of nltk\n",
    "With TreebankWordDetokenizer the tokens stored in a list after tokenizing will be combined again to a str. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f69531-5355-44da-a5fb-5758c2c5bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized text: \n",
      "['(', '7', ')', 'On', 'specific', 'query', 'by', 'the', 'Bench', 'about', 'an', 'entry', 'of', 'Rs.', '1,31,37,500', 'on', 'deposit', 'side', 'of', 'Hongkong', 'Bank', 'account', 'of', 'which', 'a', 'photo', 'copy', 'is', 'appearing', 'at', 'p.', '40', 'of', 'assessee', \"'s\", 'paper', 'book', ',', 'learned', 'authorised', 'representative', 'submitted', 'that', 'it', 'was', 'related', 'to', 'loan', 'from', 'broker', ',', 'Rahul', '&', 'Co.', 'on', 'the', 'basis', 'of', 'his', 'submission', 'a', 'necessary', 'mark', 'is', 'put', 'by', 'us', 'on', 'that', 'photo', 'copy', '.']\n",
      "\n",
      "recombined text with detokenizer: \n",
      "(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\n"
     ]
    }
   ],
   "source": [
    "twt = TreebankWordTokenizer()\n",
    "twd = TreebankWordDetokenizer()\n",
    "try_text, try_label = return_text_and_label(json_object_train[0])\n",
    "tokens = twt.tokenize(try_text)\n",
    "print(f\"tokenized text: \\n{tokens}\\n\")\n",
    "print(f\"recombined text with detokenizer: \\n{twd.detokenize(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ca18d-a2fa-4217-9305-889ea33e2a93",
   "metadata": {},
   "source": [
    "### span_tokenize\n",
    "span_tokenize is a special function provided by nltk. <br>\n",
    "It return the start and end of each token (according to the number sequence of characters) in a list. <br>\n",
    "It is especially useful to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc9106a-3e36-4d31-9be9-1f2048c33288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 3), (3, 4), (4, 5), (6, 8), (9, 17), (18, 23), (24, 26), (27, 30), (31, 36), (37, 42), (43, 45), (46, 51), (52, 54), (55, 58), (59, 70), (71, 73), (74, 81), (82, 86), (87, 89), (90, 98), (99, 103), (104, 111), (112, 114), (115, 120), (121, 122), (123, 128), (129, 133), (134, 136), (137, 146), (147, 149), (150, 152), (153, 155), (156, 158), (159, 167), (167, 169), (170, 175), (176, 180), (180, 181), (182, 189), (190, 200), (201, 215), (216, 225), (226, 230), (231, 233), (234, 237), (238, 245), (246, 248), (249, 253), (254, 258), (259, 265), (265, 266), (267, 272), (273, 274), (275, 278), (279, 281), (282, 285), (286, 291), (292, 294), (295, 298), (299, 309), (310, 311), (312, 321), (322, 326), (327, 329), (330, 333), (334, 336), (337, 339), (340, 342), (343, 347), (348, 353), (354, 358), (358, 359)]\n"
     ]
    }
   ],
   "source": [
    "try_tokens = list(TreebankWordTokenizer().span_tokenize(try_text))\n",
    "print(try_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876103d-dcf9-4e24-be04-3dc5a0c9d8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bee28d5e-de80-4f3d-830a-573050b34b4d",
   "metadata": {},
   "source": [
    "### add_label_to_tokens\n",
    "This function is crucial in the processing of the raw data. <br>\n",
    "In the tradition of a NER (Named Entity Recognition) task, tokens should be tabbed not only by their labels, but also by their positions in the label. <br>\n",
    "When a token does not lay in any label, it should be tabbed als <b> \"o\" </b>. (\"outsider\") <br>\n",
    "When it lays at the beginning of a label, then <b> \"B-\" </b> (\"beginning\") plus the name of the label. <br>\n",
    "All tokens after the first token in a label should be tabbed as \"insider\". (<b> \"I-\" </b>) <br> \n",
    "\n",
    "<i> e. g. <br>\n",
    "\" ... of Hongkong Bank ... \" <br>\n",
    "[\"o\", \"B-ORG\", \"I-ORG\"]</i> <br> <br>\n",
    "\n",
    "<i> special attention:</i> <br>\n",
    "The first parameter of this function \"<b>tokens</b>\" requires a list of <b>token spans</b>, which are the products of a span_tokenizer. <br> <br>\n",
    "\n",
    "<i> maximal span strategy </i> <br>\n",
    "Because there is no guarantee that the tokenizer could always produce the same tokenizing as the one used by annotation. <br> \n",
    "And it is also possible, that some labels of the annotation does not correspond to the boundaries of (natural) tokens because of carelessness or different understanding of the boundaries. <br>\n",
    "The maximal span strategy maximizes the included tokens in a label. So lang as a single character in the token is included in the label span, it will be labelled. <br> <br>\n",
    "    \n",
    "Later the quality of tokenizing and maximal span strategy will be checked with the <b>compare_label_with_labelled_tokens</b> function. <br>\n",
    "It will prove that the maximal span strategy would not almost change a single label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc762582-d4dd-4864-872f-d7475daf46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_to_tokens(tokens, labels):\n",
    "    # at first, create a list of \"o\"s with the same length as the number of tokens in the sentence.\n",
    "    # Hier the parameter tokens requires a list of token spans, which are the products of a span_tokenizer.\n",
    "    token_labels = [\"o\" for token in tokens]\n",
    "    \n",
    "    # afterwards, search the tokens inside of the labels.\n",
    "    # This process is not very efficient.\n",
    "    # For each label in all labels it will interate all token spans in the sentence to find out which tokens belongs to this label.\n",
    "    for label in labels:\n",
    "        # label_start, label_end with character numbers\n",
    "        label_start = label[1]\n",
    "        label_end = label[2]\n",
    "        if label_start <= label_end:\n",
    "            for i in range(0, len(tokens)):\n",
    "                # token_start, token_end also with character numbers\n",
    "                token_start, token_end = tokens[i]\n",
    "                \n",
    "                # the first token in the label (\"Beginning\")\n",
    "                if token_start <= label_start < token_end:\n",
    "                    token_labels[i] = \"B-\" + label[0]\n",
    "                \n",
    "                # the last token in a label, if the label span does not correspond to the end of the token\n",
    "                elif token_start < label_end <= token_end:\n",
    "                    token_labels[i] = \"I-\" + label[0]\n",
    "                \n",
    "                # the following tokens after the first label (\"Insider\")\n",
    "                if label_start < token_start <=  token_end <= label_end:\n",
    "                    token_labels[i] = \"I-\" + label[0]\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21a318-e07f-40bf-9136-cf1c449f9173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae693062-6727-4c58-a32e-3d1af1e38974",
   "metadata": {},
   "source": [
    "### get_tokens_with_label\n",
    "This function is an expansion of <i>add_label_to_tokens</i>. <br>\n",
    "It wraps the <i>add_label_to_tokens</i> and provides it with the required parameters. <br>\n",
    "Besides it returns also a list of tokens from the tokenizer. (with characters, rather in spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab6a57e6-733d-43d9-9c94-ae2cbfc82ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_with_label(tree):\n",
    "    text, labels = return_text_and_label(tree)\n",
    "    twt = TreebankWordTokenizer()\n",
    "    tokens_span = list(TreebankWordTokenizer().span_tokenize(text))\n",
    "    list_of_tokenized_text = twt.tokenize(text)\n",
    "    list_of_label_of_each_token = add_label_to_tokens(tokens_span, labels)\n",
    "    \n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for i in range(len(tokens_span)):\n",
    "        d[ tokens_span[i] ] = tokenized_text[i]\n",
    "    for key, value in d.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return  list_of_label_of_each_token, list_of_tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd0de7-ecfa-4d16-ab31-3d55a58696c9",
   "metadata": {},
   "source": [
    "<br> print out the labels and tokens in the first tree with <i> get_tokens_with_label </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dadcb697-bf50-4f2d-9a64-7457f02dd5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_of_label_of_each_token: \n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'B-ORG', 'I-ORG', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'B-ORG', 'I-ORG', 'I-ORG', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "\n",
      "list_of_tokenized_text: \n",
      "['(', '7', ')', 'On', 'specific', 'query', 'by', 'the', 'Bench', 'about', 'an', 'entry', 'of', 'Rs.', '1,31,37,500', 'on', 'deposit', 'side', 'of', 'Hongkong', 'Bank', 'account', 'of', 'which', 'a', 'photo', 'copy', 'is', 'appearing', 'at', 'p.', '40', 'of', 'assessee', \"'s\", 'paper', 'book', ',', 'learned', 'authorised', 'representative', 'submitted', 'that', 'it', 'was', 'related', 'to', 'loan', 'from', 'broker', ',', 'Rahul', '&', 'Co.', 'on', 'the', 'basis', 'of', 'his', 'submission', 'a', 'necessary', 'mark', 'is', 'put', 'by', 'us', 'on', 'that', 'photo', 'copy', '.']\n"
     ]
    }
   ],
   "source": [
    "list_of_label_of_each_token, list_of_tokens = get_tokens_with_label(json_object_train[0])\n",
    "print(f\"list_of_label_of_each_token: \\n{list_of_label_of_each_token}\\n\")\n",
    "print(f\"list_of_tokenized_text: \\n{list_of_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05818564-a683-491b-b6c4-e6eab6e27ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92131af5-4344-4f94-b5e8-1e324fc36b22",
   "metadata": {},
   "source": [
    "### compare_label_with_labelled_tokens\n",
    "The functions compares the labelles tokens with the labelled text from the annotation to check the quality of Tokenizing. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00313c69-d39c-4cc5-aad5-7e0b13bedeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_with_labelled_tokens(tree, object_number, print_the_differences = False):\n",
    "    number_of_errors = 0\n",
    "    error_report = \"\"\n",
    "    \n",
    "    # labels_with_text : a list of named entities directly extracted from the text according to the annotation.\n",
    "    text, labels = return_text_and_label(tree)\n",
    "    labels_with_text = []\n",
    "    for label in labels:\n",
    "        labels_with_text.append(text[label[1]:label[2]])\n",
    "    \n",
    "    # all_labelled_tokens: a list of labelled texts after tokenizing with the maximize span strategy.\n",
    "    labelled_tokens, tokenized_text = get_tokens_with_label(tree)\n",
    "    all_labelled_tokens = []\n",
    "    l = len(labelled_tokens)\n",
    "    for i in range(l):\n",
    "        single_label = []\n",
    "        if labelled_tokens[i].startswith(\"B\"):\n",
    "            single_label.append(tokenized_text[i])\n",
    "            while i+ 1 < l and labelled_tokens[i+1].startswith(\"I\"):\n",
    "                single_label.append(tokenized_text[i+1])\n",
    "                i += 1\n",
    "        if len(single_label) > 0:\n",
    "            all_labelled_tokens.append(\" \".join(single_label))\n",
    "    \n",
    "    # compare whether the length of two lists (number of labels in a sentence) are the same.\n",
    "    if len(labels_with_text) != len(all_labelled_tokens):\n",
    "        number_of_errors += abs( len(labels_with_text) - len(all_labelled_tokens) )\n",
    "        error_report += \"--------------\\n\"\n",
    "        error_report += f\"different number of labels at the {object_number}th object!\\n\"\n",
    "        error_report += f\"labels: {labels}\\n\"\n",
    "        error_report += f\"labels_with_text: {labels_with_text}\\n\"\n",
    "        error_report += f\"all_labelled_tokens: {all_labelled_tokens} \\n\"\n",
    "        error_report += \"--------------\\n\"\n",
    "    \n",
    "    # compare each element in both lists.\n",
    "    else:\n",
    "        for i in range(len(labels_with_text)):\n",
    "            gold = labels_with_text[i].replace(\" \", \"\")\n",
    "            tokenized = labels_with_text[i].replace(\" \", \"\")\n",
    "            if gold != tokenized:\n",
    "                number_of_errors += 1\n",
    "                error_report += \"--------------\\n\"\n",
    "                error_report += f\"different labels at the {object_number}th object!\\n\"\n",
    "                error_report += \"potential tokenizing problem: \"\n",
    "                error_report += f\"gold: {gold} -- tokenized: {tokenized}\"\n",
    "                error_report += \"--------------\"\n",
    "    \n",
    "    if print_the_differences:\n",
    "        print(error_report)\n",
    "    \n",
    "    return number_of_errors, error_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31051b-ecfa-473a-8c8f-cc9d40835bff",
   "metadata": {},
   "source": [
    "<br>\n",
    "compare the annotation and labelled texts after tokenizing in a single object (1328)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b93778cb-f344-4b8a-b2d5-50b377fdd089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "different number of labels at the 1328th object!\n",
      "labels: [['OTHER_PERSON', 3, 22], ['CASE_NUMBER', 28, 41], ['CASE_NUMBER', 61, 75], ['COURT', 83, 140], ['ORG', 148, 166], ['CASE_NUMBER', 166, 167], ['CASE_NUMBER', 167, 176]]\n",
      "labels_with_text: ['Jeevan Bheemmanagar', 'Cr..No.179/05', 'CC No.22109/06', '10th Addl. Chief Metropolitan Magistrate Court, Bangalore', 'Koramangala P.S.Cr', '.', 'No.430/05']\n",
      "all_labelled_tokens: ['Jeevan Bheemmanagar', 'Cr..No.179/05', 'CC No.22109/06', '10th Addl. Chief Metropolitan Magistrate Court , Bangalore.', 'Koramangala', 'P.S.Cr.No.430/05'] \n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_of_errors_1328, error_report_1328 = compare_label_with_labelled_tokens(json_object_train[1328], object_number=1328, print_the_differences = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c7107b0-2eba-4f38-8c7a-c2ce3f9d3451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_errors_1328"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ba6de-48f8-484f-a752-b9fdef08e400",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "a tokenizing error at the end of the sentence: <br>\n",
    "annoatation: 'Koramangala P.S.Cr', '.', 'No.430/05'\n",
    "tokenized text : 'Koramangala', 'P.S.Cr.No.430/05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7600312-a577-4ac0-8e26-b664c78b432f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d2068fc-3d58-46e1-ba84-4e7410f0fc45",
   "metadata": {},
   "source": [
    "### all wrong labelled entities after tokenizing in the training and development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0a06843-a0ab-46e6-812e-c6c8f0d2b67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of wrong tokenized labels in the training dataset: 5\n"
     ]
    }
   ],
   "source": [
    "with open(\"tokenizing_report_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    number_of_errors = 0\n",
    "    error_report = \"\"\n",
    "    for i in range(len(json_object_train)):\n",
    "        new_errors, new_error_report = compare_label_with_labelled_tokens(json_object_train[i], object_number = i)\n",
    "        number_of_errors += new_errors\n",
    "        error_report += new_error_report\n",
    "    f.write(error_report)\n",
    "    print(f\"total number of wrong tokenized labels in the training dataset: {number_of_errors}\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8538c32-451e-440a-b393-ad73c0e8902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of wrong tokenized labels in the developing dataset: 1\n"
     ]
    }
   ],
   "source": [
    "with open(\"tokenizing_report_dev.txt\", \"w\", encoding=\"utf-8\") as d:\n",
    "    number_of_errors = 0\n",
    "    error_report = \"\"\n",
    "    for i in range(len(json_object_dev)):\n",
    "        new_errors, new_error_report = compare_label_with_labelled_tokens(json_object_dev[i], object_number = i)\n",
    "        number_of_errors += new_errors\n",
    "        error_report += new_error_report\n",
    "    d.write(error_report)\n",
    "    print(f\"total number of wrong tokenized labels in the developing dataset: {number_of_errors}\")\n",
    "d.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a9335-364a-4d88-8055-4c98e3023cac",
   "metadata": {},
   "source": [
    "## Quality Analysis of the tokenizing\n",
    "The total number of wrong tokenized labels in both training and developing dataset are very low. <br>\n",
    "This proves the gut quality of tokenizing. <br>\n",
    "Among the 57966 labels in the training dataset are only 5 of them false tokenized. <br>\n",
    "(Tokenizing accuracy = 99.9914 %) \n",
    "\n",
    "4 of the total 6 errors are caused by the dashes in the names. <br>\n",
    "<i>labels_with_text: [ ... 'Bangalore', 'Madras'] <br>\n",
    "all_labelled_tokens: [ .. 'Bangalore-Madras'] </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21947a-a250-4911-a47f-52dfcebecdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f043635e-19d7-4191-8b4f-c05a7a7550b3",
   "metadata": {},
   "source": [
    "## Convert the json to dataframe\n",
    "As preparation of the POS-tagging in next step, the sentence number of each token will also be stored in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e4c6a7a-52ce-4ffa-a894-6c82b34083ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(trees):\n",
    "    token_and_labels = []\n",
    "    for n in range(len(trees)):\n",
    "        labels, tokens = get_tokens_with_label(trees[n])\n",
    "        if len(labels) != len(tokens):\n",
    "            raise ValueError\n",
    "        else:\n",
    "            for i in range(len(labels)):\n",
    "                token_and_labels.append([ n, tokens[i], labels[i]])\n",
    "    df = pd.DataFrame(token_and_labels)\n",
    "    df.columns = ['SentenceNR', 'Token', 'Label']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32a734c7-98a0-40ee-852a-4a7f81c7d3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceNR</th>\n",
       "      <th>Token</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>)</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>On</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>specific</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349072</th>\n",
       "      <td>9434</td>\n",
       "      <td>accused</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349073</th>\n",
       "      <td>9434</td>\n",
       "      <td>No.1</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349074</th>\n",
       "      <td>9434</td>\n",
       "      <td>as</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349075</th>\n",
       "      <td>9434</td>\n",
       "      <td>aforementioned</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349076</th>\n",
       "      <td>9434</td>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349077 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SentenceNR           Token Label\n",
       "0                0               (     o\n",
       "1                0               7     o\n",
       "2                0               )     o\n",
       "3                0              On     o\n",
       "4                0        specific     o\n",
       "...            ...             ...   ...\n",
       "349072        9434         accused     o\n",
       "349073        9434            No.1     o\n",
       "349074        9434              as     o\n",
       "349075        9434  aforementioned     o\n",
       "349076        9434               .     o\n",
       "\n",
       "[349077 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = json_to_df(json_object_train)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc5e5cc1-8a63-4e32-af02-2d27d512533b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceNR</th>\n",
       "      <th>Token</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>our</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Constitution</td>\n",
       "      <td>B-STATUTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>has</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37450</th>\n",
       "      <td>948</td>\n",
       "      <td>of</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37451</th>\n",
       "      <td>948</td>\n",
       "      <td>right</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37452</th>\n",
       "      <td>948</td>\n",
       "      <td>ear</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37453</th>\n",
       "      <td>948</td>\n",
       "      <td>lobule</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37454</th>\n",
       "      <td>948</td>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37455 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SentenceNR         Token      Label\n",
       "0               0          True          o\n",
       "1               0             ,          o\n",
       "2               0           our          o\n",
       "3               0  Constitution  B-STATUTE\n",
       "4               0           has          o\n",
       "...           ...           ...        ...\n",
       "37450         948            of          o\n",
       "37451         948         right          o\n",
       "37452         948           ear          o\n",
       "37453         948        lobule          o\n",
       "37454         948             .          o\n",
       "\n",
       "[37455 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = json_to_df(json_object_dev)\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfae057",
   "metadata": {},
   "source": [
    "## store the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a2ed4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"tokenized_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d5f3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.to_csv(\"tokenized_dev.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d068f7",
   "metadata": {},
   "source": [
    "## load the dataframes again to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ddff3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceNR</th>\n",
       "      <th>Token</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>)</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>On</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>specific</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349072</th>\n",
       "      <td>9434</td>\n",
       "      <td>accused</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349073</th>\n",
       "      <td>9434</td>\n",
       "      <td>No.1</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349074</th>\n",
       "      <td>9434</td>\n",
       "      <td>as</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349075</th>\n",
       "      <td>9434</td>\n",
       "      <td>aforementioned</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349076</th>\n",
       "      <td>9434</td>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349077 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SentenceNR           Token Label\n",
       "0                0               (     o\n",
       "1                0               7     o\n",
       "2                0               )     o\n",
       "3                0              On     o\n",
       "4                0        specific     o\n",
       "...            ...             ...   ...\n",
       "349072        9434         accused     o\n",
       "349073        9434            No.1     o\n",
       "349074        9434              as     o\n",
       "349075        9434  aforementioned     o\n",
       "349076        9434               .     o\n",
       "\n",
       "[349077 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"tokenized_train.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44e2461d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceNR</th>\n",
       "      <th>Token</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>our</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Constitution</td>\n",
       "      <td>B-STATUTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>has</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37450</th>\n",
       "      <td>948</td>\n",
       "      <td>of</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37451</th>\n",
       "      <td>948</td>\n",
       "      <td>right</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37452</th>\n",
       "      <td>948</td>\n",
       "      <td>ear</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37453</th>\n",
       "      <td>948</td>\n",
       "      <td>lobule</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37454</th>\n",
       "      <td>948</td>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37455 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SentenceNR         Token      Label\n",
       "0               0          True          o\n",
       "1               0             ,          o\n",
       "2               0           our          o\n",
       "3               0  Constitution  B-STATUTE\n",
       "4               0           has          o\n",
       "...           ...           ...        ...\n",
       "37450         948            of          o\n",
       "37451         948         right          o\n",
       "37452         948           ear          o\n",
       "37453         948        lobule          o\n",
       "37454         948             .          o\n",
       "\n",
       "[37455 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.read_csv(\"tokenized_dev.csv\")\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0114bb2-57e1-4513-8863-f61a6de4f23e",
   "metadata": {},
   "source": [
    "## How good will it work right now?\n",
    "With a simple <i>Perceptron</i> modell from <i>Sklearn</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "179de56c-57cc-450b-a0f1-dbda55c29ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349077, 27286) (349077,)\n",
      "(37455, 27286) (37455,)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train.drop([\"Label\", \"SentenceNR\"], axis = 1)\n",
    "v = DictVectorizer(sparse=True)\n",
    "X_train = v.fit_transform(X_train.to_dict('records'))\n",
    "y_train = df_train[\"Label\"]\n",
    "\n",
    "X_dev = df_dev.drop([\"Label\", \"SentenceNR\"], axis=1)\n",
    "X_dev = v.transform(X_dev.to_dict('records'))\n",
    "y_dev = df_dev[\"Label\"]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_dev.shape, y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a0b2976-15ac-4296-84b1-174cc74c6535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'B-ORG', 'I-ORG', 'B-OTHER_PERSON', 'I-OTHER_PERSON', 'B-WITNESS', 'I-WITNESS', 'B-GPE', 'B-STATUTE', 'B-DATE', 'I-DATE', 'B-PROVISION', 'I-PROVISION', 'I-STATUTE', 'B-COURT', 'I-COURT', 'B-PRECEDENT', 'I-PRECEDENT', 'B-CASE_NUMBER', 'I-CASE_NUMBER', 'I-GPE', 'B-PETITIONER', 'I-PETITIONER', 'B-JUDGE', 'I-JUDGE', 'B-RESPONDENT', 'I-RESPONDENT']\n"
     ]
    }
   ],
   "source": [
    "classes = df_train[\"Label\"].unique().tolist()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84845fca-2348-4aab-ae45-f800778b41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 9.85, NNZs: 97, Bias: -0.070000, T: 349077, Avg. loss: 0.001413\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 31.54, NNZs: 995, Bias: -0.010000, T: 349077, Avg. loss: 0.001695\n",
      "Total training time: 0.38 seconds.\n",
      "Norm: 49.06, NNZs: 2407, Bias: -0.010000, T: 349077, Avg. loss: 0.000801\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 21.59, NNZs: 466, Bias: -0.060000, T: 349077, Avg. loss: 0.001467\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 22.45, NNZs: 504, Bias: -0.020000, T: 349077, Avg. loss: 0.000592\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 30.87, NNZs: 953, Bias: -0.010000, T: 349077, Avg. loss: 0.001874\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 21.42, NNZs: 459, Bias: -0.030000, T: 349077, Avg. loss: 0.000576\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 50.86, NNZs: 2587, Bias: -0.010000, T: 349077, Avg. loss: 0.002511\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 32.63, NNZs: 1065, Bias: -0.010000, T: 349077, Avg. loss: 0.002090\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 14.25, NNZs: 203, Bias: -0.090000, T: 349077, Avg. loss: 0.001243\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.61, NNZs: 310, Bias: -0.060000, T: 349077, Avg. loss: 0.000435\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 30.48, NNZs: 929, Bias: -0.010000, T: 349077, Avg. loss: 0.000959\n",
      "Total training time: 0.24 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1Norm: 18.25, NNZs: 333, Bias: -0.050000, T: 349077, Avg. loss: 0.002001\n",
      "Total training time: 0.33 seconds.\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 40.91, NNZs: 1674, Bias: 0.000000, T: 349077, Avg. loss: 0.007197\n",
      "Total training time: 0.22 seconds.\n",
      "Norm: 16.76, NNZs: 281, Bias: -0.030000, T: 349077, Avg. loss: 0.005235\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 10.44, NNZs: 109, Bias: -0.010000, T: 349077, Avg. loss: 0.000580\n",
      "Total training time: 0.21 seconds.\n",
      "Norm: 12.00, NNZs: 144, Bias: -0.080000, T: 349077, Avg. loss: 0.004115\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 15.68, NNZs: 246, Bias: -0.020000, T: 349077, Avg. loss: 0.000611\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 30.40, NNZs: 924, Bias: -0.020000, T: 349077, Avg. loss: 0.005609\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 32.86, NNZs: 1080, Bias: -0.040000, T: 349077, Avg. loss: 0.003068\n",
      "Total training time: 0.21 seconds.\n",
      "Norm: 63.56, NNZs: 4040, Bias: -0.040000, T: 349077, Avg. loss: 0.022261\n",
      "Total training time: 0.22 seconds.\n",
      "Norm: 12.08, NNZs: 146, Bias: -0.040000, T: 349077, Avg. loss: 0.000812\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 31.02, NNZs: 962, Bias: -0.080000, T: 349077, Avg. loss: 0.011708\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 out of  27 | elapsed:    1.9s remaining:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 15.62, NNZs: 244, Bias: -0.020000, T: 349077, Avg. loss: 0.000912\n",
      "Total training time: 0.18 seconds.\n",
      "Norm: 23.87, NNZs: 570, Bias: -0.080000, T: 349077, Avg. loss: 0.006616\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 20.57, NNZs: 423, Bias: -0.050000, T: 349077, Avg. loss: 0.001282\n",
      "Total training time: 0.17 seconds.\n",
      "Norm: 129.12, NNZs: 16671, Bias: -0.010000, T: 349077, Avg. loss: 0.042169\n",
      "Total training time: 0.20 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(max_iter=5, n_jobs=-1, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(max_iter=5, n_jobs=-1, verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron(max_iter=5, n_jobs=-1, verbose=10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per = Perceptron(verbose=10, n_jobs=-1, max_iter=5)\n",
    "per.partial_fit(X_train, y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "438eeadb-9c18-456b-88dd-b6d6f9169640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "         B-ORG       0.24      0.22      0.23       159\n",
      "         I-ORG       0.24      0.10      0.14       342\n",
      "B-OTHER_PERSON       0.32      0.17      0.23       276\n",
      "I-OTHER_PERSON       0.37      0.17      0.24       195\n",
      "     B-WITNESS       0.02      0.02      0.02        58\n",
      "     I-WITNESS       0.18      0.04      0.06        54\n",
      "         B-GPE       0.09      0.25      0.13       182\n",
      "     B-STATUTE       0.54      0.37      0.44       222\n",
      "        B-DATE       0.12      0.09      0.10       222\n",
      "        I-DATE       0.39      0.15      0.22       132\n",
      "   B-PROVISION       0.87      0.70      0.78       258\n",
      "   I-PROVISION       0.48      0.16      0.24       772\n",
      "     I-STATUTE       0.41      0.10      0.17       458\n",
      "       B-COURT       0.87      0.60      0.71       178\n",
      "       I-COURT       0.15      0.05      0.07       354\n",
      "   B-PRECEDENT       0.14      0.05      0.07       177\n",
      "   I-PRECEDENT       0.43      0.35      0.39      2223\n",
      " B-CASE_NUMBER       0.59      0.26      0.37       121\n",
      " I-CASE_NUMBER       0.04      0.62      0.07       381\n",
      "         I-GPE       0.07      0.36      0.12        47\n",
      "  B-PETITIONER       0.00      0.00      0.00         9\n",
      "  I-PETITIONER       0.00      0.00      0.00        11\n",
      "       B-JUDGE       0.05      0.12      0.07         8\n",
      "       I-JUDGE       0.12      0.29      0.17         7\n",
      "  B-RESPONDENT       0.00      0.00      0.00         5\n",
      "  I-RESPONDENT       0.00      0.00      0.00        12\n",
      "\n",
      "     micro avg       0.17      0.27      0.21      6863\n",
      "     macro avg       0.26      0.20      0.19      6863\n",
      "  weighted avg       0.38      0.27      0.28      6863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes.remove(\"o\")\n",
    "print(classification_report(y_pred=per.predict(X_dev), y_true=y_dev, labels=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279632ae-351b-48dc-824d-116642fa3d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd50853-0637-43e0-b6d6-cd6962cdf750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
