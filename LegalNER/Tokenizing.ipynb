{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca87838-9071-4316-9291-263d31883b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "33b64963-8174-4209-83a6-5dc4462dad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce2d8d5-7809-4c3b-aed3-3b904dd4ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NER_TRAIN/NER_TRAIN_JUDGEMENT.json\") as json_file_train:\n",
    "    json_object_train = json.load(json_file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bb06e5-c7cf-4cfd-ab27-629df0fa81c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_and_end_and_labels(tree):\n",
    "    start_and_end_and_labels = []\n",
    "    for label in tree[\"annotations\"][0][\"result\"]:\n",
    "        labels = label[\"value\"][\"labels\"][0]\n",
    "        start = label[\"value\"][\"start\"]\n",
    "        end = label[\"value\"][\"end\"]\n",
    "        start_and_end_and_labels.append([labels, start, end])\n",
    "    return start_and_end_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0f8b16-c75a-4871-a95d-c6f8fc62d9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ORG', 90, 103], ['ORG', 267, 278]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_start_and_end_and_labels(json_object_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb67e6b3-0a6e-4b97-8588-d5ac8bdf3218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\n",
      "[['ORG', 90, 103], ['ORG', 267, 278]]\n",
      "Hongkong Bank\n",
      "Rahul & Co.\n"
     ]
    }
   ],
   "source": [
    "def print_try_text_and_label(n):\n",
    "    try_text = json_object_train[n][\"data\"][\"text\"]\n",
    "    try_label = get_start_and_end_and_labels(json_object_train[n])\n",
    "    print(try_text)\n",
    "    print(try_label)\n",
    "    for label in try_label:\n",
    "        print(try_text[label[1]: label[2]])\n",
    "print_try_text_and_label(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fada8060-556c-4941-91f0-5e08b09a4983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"\\n\\n(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\", [['ORG', 90, 103], ['ORG', 267, 278]])\n"
     ]
    }
   ],
   "source": [
    "def return_text_and_label(tree):\n",
    "    text = tree[\"data\"][\"text\"]\n",
    "    labels = get_start_and_end_and_labels(tree)\n",
    "    return text, labels\n",
    "print(return_text_and_label(json_object_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6c2b2ab2-6ab5-48c5-b65f-15b55d032422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', '7', ')', 'On', 'specific', 'query', 'by', 'the', 'Bench', 'about', 'an', 'entry', 'of', 'Rs.', '1,31,37,500', 'on', 'deposit', 'side', 'of', 'Hongkong', 'Bank', 'account', 'of', 'which', 'a', 'photo', 'copy', 'is', 'appearing', 'at', 'p.', '40', 'of', 'assessee', \"'s\", 'paper', 'book', ',', 'learned', 'authorised', 'representative', 'submitted', 'that', 'it', 'was', 'related', 'to', 'loan', 'from', 'broker', ',', 'Rahul', '&', 'Co.', 'on', 'the', 'basis', 'of', 'his', 'submission', 'a', 'necessary', 'mark', 'is', 'put', 'by', 'us', 'on', 'that', 'photo', 'copy', '.']\n",
      "(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, TreebankWordDetokenizer\n",
    "twt = TreebankWordTokenizer()\n",
    "twd = TreebankWordDetokenizer()\n",
    "try_text, try_label = return_text_and_label(json_object_train[0])\n",
    "#token_number_list = list(twt().span_tokenize(try_text))\n",
    "tokens = twt.tokenize(try_text)\n",
    "print(tokens)\n",
    "print(twd.detokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "01d5552b-f5f3-4126-99f6-d19c010929cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 3), (3, 4), (4, 5), (6, 8), (9, 17), (18, 23), (24, 26), (27, 30), (31, 36), (37, 42), (43, 45), (46, 51), (52, 54), (55, 58), (59, 70), (71, 73), (74, 81), (82, 86), (87, 89), (90, 98), (99, 103), (104, 111), (112, 114), (115, 120), (121, 122), (123, 128), (129, 133), (134, 136), (137, 146), (147, 149), (150, 152), (153, 155), (156, 158), (159, 167), (167, 169), (170, 175), (176, 180), (180, 181), (182, 189), (190, 200), (201, 215), (216, 225), (226, 230), (231, 233), (234, 237), (238, 245), (246, 248), (249, 253), (254, 258), (259, 265), (265, 266), (267, 272), (273, 274), (275, 278), (279, 281), (282, 285), (286, 291), (292, 294), (295, 298), (299, 309), (310, 311), (312, 321), (322, 326), (327, 329), (330, 333), (334, 336), (337, 339), (340, 342), (343, 347), (348, 353), (354, 358), (358, 359)]\n"
     ]
    }
   ],
   "source": [
    "try_tokens = list(TreebankWordTokenizer().span_tokenize(try_text))\n",
    "print(try_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "7c76eab0-776e-4d7e-ab3b-9fc74b4b05c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorrect_span = [list(token) for token in try_tokens]\\nfor i in range(len(correct_span) - 1):\\n    if correct_span[i+1][0] == correct_span[i][1]:\\n        correct_span[i+1][0] += 1\\n'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "correct_span = [list(token) for token in try_tokens]\n",
    "for i in range(len(correct_span) - 1):\n",
    "    if correct_span[i+1][0] == correct_span[i][1]:\n",
    "        correct_span[i+1][0] += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05818564-a683-491b-b6c4-e6eab6e27ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6f2ae928-7cc1-4e8a-812d-426cb55e1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_to_tokens(tokens, labels):\n",
    "    token_labels = [\"o\" for token in tokens]\n",
    "    for label in labels:\n",
    "        label_start = label[1]\n",
    "        label_end = label[2]\n",
    "        if label_start <= label_end:\n",
    "            for i in range(0, len(tokens)):\n",
    "                token_start, token_end = tokens[i]\n",
    "                \n",
    "                # the first token in the label (\"Beginning\")\n",
    "                if token_start <= label_start < token_end:\n",
    "                    token_labels[i] = \"B-\" + label[0]\n",
    "                \n",
    "                # the last token in a label, if the label span does not correspond to the end of the token\n",
    "                #elif token_start <= label_end <= token_end:\n",
    "                elif token_start < label_end <= token_end:\n",
    "                    token_labels[i] = \"I-\" + label[0]\n",
    "                \n",
    "                # the following tokens after the first label (\"Insider\")\n",
    "                if label_start < token_start <=  token_end <= label_end:\n",
    "                    token_labels[i] = \"I-\" + label[0]\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "468d97ce-aa04-48ae-8108-f751b94f09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_with_label(tree):\n",
    "    text, labels = return_text_and_label(tree)\n",
    "    twt = TreebankWordTokenizer()\n",
    "    tokens_span = list(TreebankWordTokenizer().span_tokenize(text))\n",
    "    tokenized_text = twt.tokenize(text)\n",
    "    \n",
    "    #d = {}\n",
    "    #for i in range(len(tokens_span)):\n",
    "    #    d[ tokens_span[i] ] = tokenized_text[i]\n",
    "    #for key, value in d.items():\n",
    "    #    print(f\"{key}: {value}\")\n",
    "    return add_label_to_tokens(tokens_span, labels), tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "78254188-638b-4774-b2d5-7f9dfd5b43b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['o', 'o', 'o', 'o', 'o', 'B-OTHER_PERSON', 'I-OTHER_PERSON', 'o', 'o', 'o', 'o', 'o', 'B-GPE', 'o', 'o', 'o', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'B-DATE', 'o'], ['(', '3', ')', 'that', 'M/s', 'Sureshchand', 'Isarchand', 'got', 'this', 'vehicle', 'replaced', 'on', 'Bavana-Dholpur', 'route', 'and', 'the', 'Regional', 'Transport', 'Authority', ',', 'Jaipur', 'had', 'allowed', 'the', 'said', 'application', 'for', 'replacement', 'by', 'circular', 'note', 'dated', '17-12-66', '.'])\n"
     ]
    }
   ],
   "source": [
    "print(get_tokens_with_label(json_object_train[2236]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86fa6ff-7259-45c0-a395-ea7b3bfd7a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "00313c69-d39c-4cc5-aad5-7e0b13bedeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_with_labelled_tokens(tree):\n",
    "    text, labels = return_text_and_label(tree)\n",
    "    labels_with_text = []\n",
    "    for label in labels:\n",
    "        labels_with_text.append(text[label[1]:label[2]])\n",
    "    \n",
    "    labelled_tokens, tokenized_text = get_tokens_with_label(tree)\n",
    "    all_labelled_tokens = []\n",
    "    l = len(labelled_tokens)\n",
    "    for i in range(l):\n",
    "        single_label = []\n",
    "        if labelled_tokens[i].startswith(\"B\"):\n",
    "            single_label.append(tokenized_text[i])\n",
    "            while i+ 1 < l and labelled_tokens[i+1].startswith(\"I\"):\n",
    "                single_label.append(tokenized_text[i+1])\n",
    "                i += 1\n",
    "        if len(single_label) > 0:\n",
    "            all_labelled_tokens.append(\" \".join(single_label))\n",
    "    \n",
    "    # compare\n",
    "    if len(labels_with_text) != len(all_labelled_tokens):\n",
    "        print(\"different number of labels!\")\n",
    "        print(f\"labels: {labels}\")\n",
    "        print(f\"labels_with_text: {labels_with_text}\")\n",
    "        print(f\"all_labelled_tokens: {all_labelled_tokens} \\n\")\n",
    "    else:\n",
    "        for i in range(len(labels_with_text)):\n",
    "            gold = labels_with_text[i].replace(\" \", \"\")\n",
    "            tokenized = labels_with_text[i].replace(\" \", \"\")\n",
    "            if gold != tokenized:\n",
    "                print(\"potential tokenizing problem: \")\n",
    "                print(f\"gold: {gold} -- tokenized: {tokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "b0a06843-a0ab-46e6-812e-c6c8f0d2b67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different number of labels!\n",
      "labels: [['OTHER_PERSON', 3, 22], ['CASE_NUMBER', 28, 41], ['CASE_NUMBER', 61, 75], ['COURT', 83, 140], ['ORG', 148, 166], ['CASE_NUMBER', 166, 167], ['CASE_NUMBER', 167, 176]]\n",
      "labels_with_text: ['Jeevan Bheemmanagar', 'Cr..No.179/05', 'CC No.22109/06', '10th Addl. Chief Metropolitan Magistrate Court, Bangalore', 'Koramangala P.S.Cr', '.', 'No.430/05']\n",
      "all_labelled_tokens: ['Jeevan Bheemmanagar', 'Cr..No.179/05', 'CC No.22109/06', '10th Addl. Chief Metropolitan Magistrate Court , Bangalore.', 'Koramangala', 'P.S.Cr.No.430/05'] \n",
      "\n",
      "different number of labels!\n",
      "labels: [['OTHER_PERSON', 13, 34], ['GPE', 64, 70], ['GPE', 71, 78], ['ORG', 93, 129], ['DATE', 202, 210]]\n",
      "labels_with_text: ['Sureshchand Isarchand', 'Bavana', 'Dholpur', 'Regional Transport Authority, Jaipur', '17-12-66']\n",
      "all_labelled_tokens: ['Sureshchand Isarchand', 'Bavana-Dholpur', 'Regional Transport Authority , Jaipur', '17-12-66'] \n",
      "\n",
      "different number of labels!\n",
      "labels: [['GPE', 41, 47], ['GPE', 48, 52], ['GPE', 66, 81], ['ORG', 122, 151]]\n",
      "labels_with_text: ['Bombay', 'Pune', 'Nalgaon Village', 'Wadagaon-Moval Police Station']\n",
      "all_labelled_tokens: ['Bombay-Pune', 'Nalgaon Village', 'Wadagaon-Moval Police Station'] \n",
      "\n",
      "different number of labels!\n",
      "labels: [['OTHER_PERSON', 39, 51], ['OTHER_PERSON', 60, 72], ['PROVISION', 209, 215], ['PROVISION', 226, 238], ['STATUTE', 242, 245], ['PROVISION', 248, 254], ['STATUTE', 258, 261], ['PROVISION', 272, 284], ['STATUTE', 288, 291], ['PROVISION', 298, 304], ['STATUTE', 308, 311], ['PROVISION', 322, 334], ['STATUTE', 338, 341]]\n",
      "labels_with_text: ['R.M. Tuffail', 'M.C. Dhingra', 's. 364', 'Section 120B', 'IPC', 's. 302', 'IPC', 'Section 120B', 'IPC', 's. 201', 'IPC', 'Section 120B', 'IPC']\n",
      "all_labelled_tokens: ['R.M. Tuffail', 'M.C. Dhingra', 'U/s. 364', 'Section 120B', 'IPC/U/s. 302', 'IPC', 'Section 120B', 'IPC', 'U/s. 201', 'IPC', 'Section 120B', 'IPC'] \n",
      "\n",
      "different number of labels!\n",
      "labels: [['OTHER_PERSON', 114, 121], ['DATE', 204, 212], ['GPE', 217, 226], ['GPE', 227, 233]]\n",
      "labels_with_text: ['Sampath', '2.2.1988', 'Bangalore', 'Madras']\n",
      "all_labelled_tokens: ['Sampath', '2.2.1988', 'Bangalore-Madras'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(json_object_train)):\n",
    "    compare_label_with_labelled_tokens(json_object_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "554ac222-6b8d-4a7c-a81c-64c064255450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different number of labels!\n",
      "labels: [['OTHER_PERSON', 114, 121], ['DATE', 204, 212], ['GPE', 217, 226], ['GPE', 227, 233]]\n",
      "labels_with_text: ['Sampath', '2.2.1988', 'Bangalore', 'Madras']\n",
      "all_labelled_tokens: ['Sampath', '2.2.1988 ,', 'Bangalore-Madras'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_label_with_labelled_tokens(json_object_train[7417])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b87afb-b323-49c6-8c6f-d1cf9d6f5523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "2801d4fb-4728-42aa-aad4-4f98ee4b5b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'o']"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_and_labels_train = []\n",
    "for tree in json_object_train:\n",
    "    labels, tokens = get_tokens_with_label(tree)\n",
    "    if len(labels) != len(tokens):\n",
    "        print(\"BUG!\")\n",
    "    else:\n",
    "        for i in range(len(labels)):\n",
    "            token_and_labels_train.append([ tokens[i], labels[i] ])\n",
    "token_and_labels_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "6382c588-c425-4be6-9627-611e91100653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>)</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>specific</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349072</th>\n",
       "      <td>accused</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349073</th>\n",
       "      <td>No.1</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349074</th>\n",
       "      <td>as</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349075</th>\n",
       "      <td>aforementioned</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349076</th>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349077 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0  1\n",
       "0                    (  o\n",
       "1                    7  o\n",
       "2                    )  o\n",
       "3                   On  o\n",
       "4             specific  o\n",
       "...                ... ..\n",
       "349072         accused  o\n",
       "349073            No.1  o\n",
       "349074              as  o\n",
       "349075  aforementioned  o\n",
       "349076               .  o\n",
       "\n",
       "[349077 rows x 2 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(token_and_labels_train)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "2e3ae734-3f28-4c08-9421-0ac78d7fc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NER_DEV/NER_DEV/NER_DEV_JUDGEMENT.json\") as json_file_dev:\n",
    "    json_object_dev = json.load(json_file_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "2a778193-a487-4877-80eb-95a5646a5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different number of labels!\n",
      "labels: [['GPE', 138, 144], ['GPE', 145, 149], ['OTHER_PERSON', 239, 248], ['OTHER_PERSON', 260, 276]]\n",
      "labels_with_text: ['Bombay', 'Agra', 'Amaraveni', 'Venkateswara Rao']\n",
      "all_labelled_tokens: ['Bombay-Agra', 'Amaraveni', 'Venkateswara Rao'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(json_object_dev)):\n",
    "    compare_label_with_labelled_tokens(json_object_dev[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "fb163ad1-91dd-4989-91f9-2b62180eb0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['True', 'o'],\n",
       " [',', 'o'],\n",
       " ['our', 'o'],\n",
       " ['Constitution', 'B-STATUTE'],\n",
       " ['has', 'o'],\n",
       " ['no', 'o'],\n",
       " [\"'due\", 'o'],\n",
       " ['process', 'o'],\n",
       " [\"'\", 'o'],\n",
       " ['clause', 'o']]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_and_labels_dev = []\n",
    "for tree in json_object_dev:\n",
    "    labels, tokens = get_tokens_with_label(tree)\n",
    "    if len(labels) != len(tokens):\n",
    "        print(\"BUG!\")\n",
    "    else:\n",
    "        for i in range(len(labels)):\n",
    "            token_and_labels_dev.append([ tokens[i], labels[i] ])\n",
    "token_and_labels_dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "f8018876-9671-4f14-8ae5-ade1211c4ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>our</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Constitution</td>\n",
       "      <td>B-STATUTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>has</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37450</th>\n",
       "      <td>of</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37451</th>\n",
       "      <td>right</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37452</th>\n",
       "      <td>ear</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37453</th>\n",
       "      <td>lobule</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37454</th>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37455 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0          1\n",
       "0              True          o\n",
       "1                 ,          o\n",
       "2               our          o\n",
       "3      Constitution  B-STATUTE\n",
       "4               has          o\n",
       "...             ...        ...\n",
       "37450            of          o\n",
       "37451         right          o\n",
       "37452           ear          o\n",
       "37453        lobule          o\n",
       "37454             .          o\n",
       "\n",
       "[37455 rows x 2 columns]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.DataFrame(token_and_labels_dev)\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "179de56c-57cc-450b-a0f1-dbda55c29ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233881, 27286) (233881,)\n",
      "(115196, 27286) (115196,)\n"
     ]
    }
   ],
   "source": [
    "X = df_train.drop(1, axis=1)\n",
    "v = DictVectorizer(sparse=True)\n",
    "X = v.fit_transform(X.to_dict('records'))\n",
    "y = df_train[1]\n",
    "\n",
    "#classes = np.unique(y)\n",
    "#classes = classes.tolist()\n",
    "classes = df_train[1].unique().tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "0cf5a78d-b50c-43a7-b31e-bc0247af81d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'B-ORG', 'I-ORG', 'B-OTHER_PERSON', 'I-OTHER_PERSON', 'B-WITNESS', 'I-WITNESS', 'B-GPE', 'B-STATUTE', 'B-DATE', 'I-DATE', 'B-PROVISION', 'I-PROVISION', 'I-STATUTE', 'B-COURT', 'I-COURT', 'B-PRECEDENT', 'I-PRECEDENT', 'B-CASE_NUMBER', 'I-CASE_NUMBER', 'I-GPE', 'B-PETITIONER', 'I-PETITIONER', 'B-JUDGE', 'I-JUDGE', 'B-RESPONDENT', 'I-RESPONDENT']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "84845fca-2348-4aab-ae45-f800778b41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 8.49, NNZs: 72, Bias: -0.020000, T: 233881, Avg. loss: 0.001383\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 18.95, NNZs: 359, Bias: -0.030000, T: 233881, Avg. loss: 0.001402\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 26.85, NNZs: 721, Bias: -0.010000, T: 233881, Avg. loss: 0.001574\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 41.77, NNZs: 1745, Bias: -0.010000, T: 233881, Avg. loss: 0.000733\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 19.77, NNZs: 391, Bias: -0.010000, T: 233881, Avg. loss: 0.000540\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 43.78, NNZs: 1917, Bias: -0.010000, T: 233881, Avg. loss: 0.002377\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 18.30, NNZs: 335, Bias: -0.030000, T: 233881, Avg. loss: 0.000560\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 26.63, NNZs: 709, Bias: -0.010000, T: 233881, Avg. loss: 0.001777\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 27.78, NNZs: 772, Bias: -0.020000, T: 233881, Avg. loss: 0.002081\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 12.49, NNZs: 156, Bias: -0.040000, T: 233881, Avg. loss: 0.001173\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 16.94, NNZs: 287, Bias: -0.070000, T: 233881, Avg. loss: 0.001948\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 14.66, NNZs: 215, Bias: -0.030000, T: 233881, Avg. loss: 0.000341\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 26.04, NNZs: 678, Bias: -0.020000, T: 233881, Avg. loss: 0.000887\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 35.17, NNZs: 1237, Bias: -0.050000, T: 233881, Avg. loss: 0.006957\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 14.59, NNZs: 213, Bias: -0.090000, T: 233881, Avg. loss: 0.005185\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 10.77, NNZs: 116, Bias: -0.120000, T: 233881, Avg. loss: 0.004145\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 10.54, NNZs: 111, Bias: -0.010000, T: 233881, Avg. loss: 0.000551\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 14.07, NNZs: 198, Bias: -0.020000, T: 233881, Avg. loss: 0.000553\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 27.80, NNZs: 773, Bias: -0.030000, T: 233881, Avg. loss: 0.005449\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 28.93, NNZs: 837, Bias: -0.010000, T: 233881, Avg. loss: 0.002872\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 10.39, NNZs: 108, Bias: -0.100000, T: 233881, Avg. loss: 0.000773\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 28.67, NNZs: 822, Bias: -0.060000, T: 233881, Avg. loss: 0.011405\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 55.82, NNZs: 3116, Bias: -0.040000, T: 233881, Avg. loss: 0.021732\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 13.00, NNZs: 169, Bias: -0.030000, T: 233881, Avg. loss: 0.000941\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 21.56, NNZs: 465, Bias: -0.050000, T: 233881, Avg. loss: 0.006478\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 17.46, NNZs: 305, Bias: -0.030000, T: 233881, Avg. loss: 0.001246\n",
      "Total training time: 0.04 seconds.\n",
      "Norm: 113.72, NNZs: 12932, Bias: -0.000000, T: 233881, Avg. loss: 0.041306\n",
      "Total training time: 0.04 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 out of  27 | elapsed:    0.5s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(max_iter=5, n_jobs=-1, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(max_iter=5, n_jobs=-1, verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron(max_iter=5, n_jobs=-1, verbose=10)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per = Perceptron(verbose=10, n_jobs=-1, max_iter=5)\n",
    "per.partial_fit(X_train, y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "438eeadb-9c18-456b-88dd-b6d6f9169640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "         B-ORG       0.47      0.27      0.34       472\n",
      "         I-ORG       0.36      0.24      0.28       937\n",
      "B-OTHER_PERSON       0.40      0.22      0.28       853\n",
      "I-OTHER_PERSON       0.35      0.32      0.33       695\n",
      "     B-WITNESS       0.19      0.13      0.16       293\n",
      "     I-WITNESS       0.13      0.04      0.06       260\n",
      "         B-GPE       0.16      0.23      0.19       464\n",
      "     B-STATUTE       0.74      0.61      0.67       600\n",
      "        B-DATE       0.10      0.35      0.16       583\n",
      "        I-DATE       0.21      0.10      0.14       638\n",
      "   B-PROVISION       0.83      0.88      0.85       786\n",
      "   I-PROVISION       0.61      0.21      0.31      2185\n",
      "     I-STATUTE       0.57      0.41      0.48      1263\n",
      "       B-COURT       0.74      0.59      0.66       429\n",
      "       I-COURT       0.21      0.05      0.07       970\n",
      "   B-PRECEDENT       0.10      0.03      0.05       437\n",
      "   I-PRECEDENT       0.52      0.26      0.34      4647\n",
      " B-CASE_NUMBER       0.40      0.32      0.36       353\n",
      " I-CASE_NUMBER       0.37      0.11      0.17      1319\n",
      "         I-GPE       0.26      0.17      0.21        87\n",
      "  B-PETITIONER       0.22      0.08      0.11       143\n",
      "  I-PETITIONER       0.11      0.02      0.03       130\n",
      "       B-JUDGE       0.27      0.26      0.27       185\n",
      "       I-JUDGE       0.00      0.12      0.01       132\n",
      "  B-RESPONDENT       0.18      0.07      0.10       127\n",
      "  I-RESPONDENT       0.11      0.03      0.04       141\n",
      "\n",
      "     micro avg       0.30      0.27      0.28     19129\n",
      "     macro avg       0.33      0.24      0.26     19129\n",
      "  weighted avg       0.44      0.27      0.31     19129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes.remove(\"o\")\n",
    "print(classification_report(y_pred=per.predict(X_test), y_true=y_test, labels=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279632ae-351b-48dc-824d-116642fa3d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd50853-0637-43e0-b6d6-cd6962cdf750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab9fd0-a899-45f1-83c3-b112ba4f7f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ff1f6-359f-4f7f-a683-50ee599a21e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e02ab41b-16e5-4830-8536-36e8a24c2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_tokens_before(text, label_start):\n",
    "    tokens_before = word_tokenize(text[:label_start])\n",
    "    #if len(tokens_before) > 0:\n",
    "    #    if tokens_before[-1] == \".\":\n",
    "    #        abbr_combine = \"\".join(tokens_before[-2:])\n",
    "    #        tokens_before = tokens_before[:-2]\n",
    "    #        tokens_before.append(abbr_combine)\n",
    "    return len(tokens_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6573f319-a5d7-467d-9596-23f075d89c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_id_to_token_id(text, start, end):\n",
    "    token_number_start = number_of_tokens_before(text, start)\n",
    "    token_number_end = number_of_tokens_before(text, end)\n",
    "    return token_number_start, token_number_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2d2e066-a48a-4841-9b19-f8f9c90e3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(text, labels, check_list):\n",
    "    all_NE = [text[label[1]:label[2]] for label in labels]\n",
    "    for i in range(len(all_NE)):\n",
    "        origin = all_NE[i].replace(\" \", \"\")\n",
    "        check = \"\".join(check_list[i])\n",
    "        if origin != check:\n",
    "            print(\"Tokenizing Error!\")\n",
    "            print(\"Origin: \" , all_NE[i])\n",
    "            print(\"Error: \" , check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61c28dd8-3612-4a84-8c19-b59f9eda7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_to_text_plus_label(text, labels):\n",
    "    token_numbers_of_labels = []\n",
    "    for label in labels:\n",
    "        start, end = slice_id_to_token_id(text, label[1], label[2])\n",
    "        token_numbers_of_labels.append([label[0], start, end])\n",
    "    tokens = word_tokenize(text)\n",
    "    number_of_tokens = len(tokens)\n",
    "    \n",
    "    check_list = []\n",
    "    return_list = []\n",
    "    token_labels = [ \"o\" for i in range(number_of_tokens)]\n",
    "    for label in token_numbers_of_labels:\n",
    "        check_list.append(tokens[label[1]:label[2]])\n",
    "        # check the label\n",
    "        # print(tokens[label[1]:label[2]])\n",
    "        # print(label[1],label[2])\n",
    "        \n",
    "        #token_labels[label[1]] = named_entities_to_list[label[0]] * 2 - 1\n",
    "        token_labels[label[1]] = \"B-\" + label[0]\n",
    "        for i in range(label[1] + 1, label[2]):\n",
    "            #token_labels[i] = named_entities_to_list[label[0]]  * 2\n",
    "            token_labels[i] = \"I-\" + label[0]\n",
    "    \n",
    "    for i in range (len(token_labels)):\n",
    "        return_list.append([tokens[i], token_labels[i]])\n",
    "    \n",
    "    check_labels(text, labels, check_list)\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f6579f8-2507-4579-99c6-432e1a77cd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Error!\n",
      "Origin:  Rahul & Co.\n",
      "Error:  Rahul&Co.on\n",
      "[['(', 'o'], ['7', 'o'], [')', 'o'], ['On', 'o'], ['specific', 'o'], ['query', 'o'], ['by', 'o'], ['the', 'o'], ['Bench', 'o'], ['about', 'o'], ['an', 'o'], ['entry', 'o'], ['of', 'o'], ['Rs', 'o'], ['.', 'o'], ['1,31,37,500', 'o'], ['on', 'o'], ['deposit', 'o'], ['side', 'o'], ['of', 'o'], ['Hongkong', 'B-ORG'], ['Bank', 'I-ORG'], ['account', 'o'], ['of', 'o'], ['which', 'o'], ['a', 'o'], ['photo', 'o'], ['copy', 'o'], ['is', 'o'], ['appearing', 'o'], ['at', 'o'], ['p.', 'o'], ['40', 'o'], ['of', 'o'], ['assessee', 'o'], [\"'s\", 'o'], ['paper', 'o'], ['book', 'o'], [',', 'o'], ['learned', 'o'], ['authorised', 'o'], ['representative', 'o'], ['submitted', 'o'], ['that', 'o'], ['it', 'o'], ['was', 'o'], ['related', 'o'], ['to', 'o'], ['loan', 'o'], ['from', 'o'], ['broker', 'o'], [',', 'o'], ['Rahul', 'B-ORG'], ['&', 'I-ORG'], ['Co.', 'I-ORG'], ['on', 'I-ORG'], ['the', 'o'], ['basis', 'o'], ['of', 'o'], ['his', 'o'], ['submission', 'o'], ['a', 'o'], ['necessary', 'o'], ['mark', 'o'], ['is', 'o'], ['put', 'o'], ['by', 'o'], ['us', 'o'], ['on', 'o'], ['that', 'o'], ['photo', 'o'], ['copy', 'o'], ['.', 'o']]\n"
     ]
    }
   ],
   "source": [
    "for n in range(0, 1):\n",
    "    try_text = json_object_train[n][\"data\"][\"text\"]\n",
    "    try_label = get_start_and_end_and_labels(json_object_train[n])\n",
    "    try_convert = text_to_text_plus_label(try_text, try_label)\n",
    "    print(try_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "23aa7fba-a33b-4333-9377-8391a371da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'had', 'prepared', 'G.D.', 'No', '.', '7', 'on', '19.8.1998', 'at', '3.05', 'A.M', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(try_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "438cf49f-1432-4d8c-ac35-21a04bb395ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['OTHER_PERSON', 17, 23], ['PROVISION', 171, 195], ['STATUTE', 203, 218]]\n",
      "[['Therefore', 'o'], [',', 'o'], ['Shri', 'o'], ['.', 'o'], ['Dharap', 'B-OTHER_PERSON'], [',', 'o'], ['learned', 'o'], ['Counsel', 'o'], ['for', 'o'], ['the', 'o'], ['respondent', 'o'], ['submitted', 'o'], ['that', 'o'], ['the', 'o'], ['learned', 'o'], ['Single', 'o'], ['Judge', 'o'], ['was', 'o'], ['justified', 'o'], ['in', 'o'], ['holding', 'o'], ['that', 'o'], ['there', 'o'], ['is', 'o'], ['an', 'o'], ['unfair', 'o'], ['labour', 'o'], ['practice', 'o'], ['under', 'o'], ['item', 'B-PROVISION'], ['4', 'I-PROVISION'], ['(', 'I-PROVISION'], ['c', 'I-PROVISION'], [')', 'I-PROVISION'], ['of', 'I-PROVISION'], ['Schedule-II', 'I-PROVISION'], ['of', 'o'], ['the', 'o'], ['MRTU', 'B-STATUTE'], ['&', 'I-STATUTE'], ['PULP', 'I-STATUTE'], ['Act', 'I-STATUTE'], ['.', 'o']]\n"
     ]
    }
   ],
   "source": [
    "try_text = json_object_train[58][\"data\"][\"text\"]\n",
    "try_label = get_start_and_end_and_labels(json_object_train[58])\n",
    "print(try_label)\n",
    "try_convert = text_to_text_plus_label(try_text, try_label)\n",
    "print(try_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3071519b-80ca-47e3-aff9-79cc6cc684df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hongkong Bank'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_text[b:e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d68bac7-b001-4349-8e9a-67cac243d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = try_label[0][1]\n",
    "e = try_label[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "54cd2f83-99b6-451d-bc2d-adc3162eba2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aeb64562-3e7f-491b-8f2b-33989facf4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Therefore', ',', 'Shri', '.', 'Dharap', ',', 'learned', 'Counsel', 'for', 'the', 'respondent', 'submitted', 'that', 'the', 'learned', 'Single', 'Judge', 'was', 'justified', 'in', 'holding', 'that', 'there', 'is', 'an', 'unfair', 'labour', 'practice', 'under', 'item', '4', '(', 'c', ')', 'of', 'Schedule-II', 'of', 'the', 'MRTU', '&', 'PULP', 'Act', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(try_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f30a12fc-33f1-4720-a0a3-61865b059a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 22)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = try_label[1][1]\n",
    "end = try_label[1][2]\n",
    "slice_id_to_token_id(try_text, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3332d016-e762-4c43-9eb6-0634ff4ebdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'affirm', 'the', 'death', 'sentence', 'passed', 'by', 'the', 'trial', 'court', 'as', 'also', 'the', 'other', 'sentences', 'passed', 'under', 'Sections', '364', 'and', '376']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(try_text[:try_label[1][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c8d13091-d2e3-45e6-9edc-233b22564e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'affirm', 'the', 'death', 'sentence', 'passed', 'by', 'the', 'trial', 'court', 'as', 'also', 'the', 'other', 'sentences', 'passed', 'under', 'Sections', '364', 'and', '376', 'I.P.C', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(try_text[:try_label[1][2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "915e63c7-6f9a-410f-bf78-8c4cf62d693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', '7', ')', 'On', 'specific', 'query', 'by', 'the', 'Bench', 'about', 'an', 'entry', 'of', 'Rs', '.', '1,31,37,500', 'on', 'deposit', 'side', 'of']\n",
      "['(', '7', ')', 'On', 'specific', 'query', 'by', 'the', 'Bench', 'about', 'an', 'entry', 'of', 'Rs', '.', '1,31,37,500', 'on', 'deposit', 'side', 'of', 'Hongkong', 'Bank']\n",
      "\n",
      "\n",
      "['(', '7', ')', 'On', 'specific', 'query', 'by', 'the', 'Bench', 'about', 'an', 'entry', 'of', 'Rs', '.', '1,31,37,500', 'on', 'deposit', 'side', 'of', 'Hongkong', 'Bank', 'account', 'of', 'which', 'a', 'photo', 'copy', 'is', 'appearing', 'at', 'p.', '40', 'of', 'assessee', \"'s\", 'paper', 'book', ',', 'learned', 'authorised', 'representative', 'submitted', 'that', 'it', 'was', 'related', 'to', 'loan', 'from', 'broker', ',']\n",
      "!\n",
      "['(', '7', ')', 'On', 'specific', 'query', 'by', 'the', 'Bench', 'about', 'an', 'entry', 'of', 'Rs', '.', '1,31,37,500', 'on', 'deposit', 'side', 'of', 'Hongkong', 'Bank', 'account', 'of', 'which', 'a', 'photo', 'copy', 'is', 'appearing', 'at', 'p.', '40', 'of', 'assessee', \"'s\", 'paper', 'book', ',', 'learned', 'authorised', 'representative', 'submitted', 'that', 'it', 'was', 'related', 'to', 'loan', 'from', 'broker', ',', 'Rahul', '&', 'Co', '.']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\n"
     ]
    }
   ],
   "source": [
    "try_text = json_object_train[0][\"data\"][\"text\"]\n",
    "try_label = get_start_and_end_and_labels(json_object_train[0])\n",
    "for n in range(len(try_label)):\n",
    "    print(tokens_before(try_text,try_label[n][1]))\n",
    "    print(tokens_before(try_text,try_label[n][2]))\n",
    "    print(\"\\n\")\n",
    "print(try_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f8a3a30-a78b-4f0a-a9b2-61a511251520",
   "metadata": {},
   "source": [
    "def tokens_before(text, label_start):\n",
    "    tokens_before = word_tokenize(text[:label_start])\n",
    "    if len(tokens_before) > 0:\n",
    "        if tokens_before[-1] == \".\":\n",
    "            print(\"!\")\n",
    "    return tokens_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428bc0c-46fb-407c-a9fb-9b0f010b3cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2610bf9-599b-4871-b5af-208d7fd9b421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "29c4e206-62ec-4fe5-be7c-66c82ff54a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\n",
      "[['ORG', 90, 103], ['ORG', 267, 278]]\n",
      "Hongkong Bank\n",
      "Rahul & Co.\n",
      "[(2, 3), (3, 4), (4, 5), (6, 8), (9, 17), (18, 23), (24, 26), (27, 30), (31, 36), (37, 42), (43, 45), (46, 51), (52, 54), (55, 58), (59, 70), (71, 73), (74, 81), (82, 86), (87, 89), (90, 98), (99, 103), (104, 111), (112, 114), (115, 120), (121, 122), (123, 128), (129, 133), (134, 136), (137, 146), (147, 149), (150, 152), (153, 155), (156, 158), (159, 167), (167, 169), (170, 175), (176, 180), (180, 181), (182, 189), (190, 200), (201, 215), (216, 225), (226, 230), (231, 233), (234, 237), (238, 245), (246, 248), (249, 253), (254, 258), (259, 265), (265, 266), (267, 272), (273, 274), (275, 278), (279, 281), (282, 285), (286, 291), (292, 294), (295, 298), (299, 309), (310, 311), (312, 321), (322, 326), (327, 329), (330, 333), (334, 336), (337, 339), (340, 342), (343, 347), (348, 353), (354, 358), (358, 359)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "print(try_text)\n",
    "print(try_label)\n",
    "for label in try_label:\n",
    "    print(try_text[label[1]: label[2]])\n",
    "\n",
    "token_number_list = list(twt().span_tokenize(try_text))\n",
    "print(token_number_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "39de989b-cb8a-4c83-8c0b-1bda6e9b2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "7\n",
      ")\n",
      "On\n",
      "specific\n",
      "query\n",
      "by\n",
      "the\n",
      "Bench\n",
      "about\n",
      "an\n",
      "entry\n",
      "of\n",
      "Rs.\n",
      "1,31,37,500\n",
      "on\n",
      "deposit\n",
      "side\n",
      "of\n",
      "Hongkong\n",
      "Bank\n",
      "account\n",
      "of\n",
      "which\n",
      "a\n",
      "photo\n",
      "copy\n",
      "is\n",
      "appearing\n",
      "at\n",
      "p.\n",
      "40\n",
      "of\n",
      "assessee\n",
      "'s\n",
      "paper\n",
      "book\n",
      ",\n",
      "learned\n",
      "authorised\n",
      "representative\n",
      "submitted\n",
      "that\n",
      "it\n",
      "was\n",
      "related\n",
      "to\n",
      "loan\n",
      "from\n",
      "broker\n",
      ",\n",
      "Rahul\n",
      "&\n",
      "Co.\n",
      "on\n",
      "the\n",
      "basis\n",
      "of\n",
      "his\n",
      "submission\n",
      "a\n",
      "necessary\n",
      "mark\n",
      "is\n",
      "put\n",
      "by\n",
      "us\n",
      "on\n",
      "that\n",
      "photo\n",
      "copy\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token_number in token_number_list:\n",
    "    print(try_text[token_number[0]: token_number[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c685d8e-aed5-49cc-b357-c42f16772d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (3, 6),\n",
       " (7, 12),\n",
       " (13, 17),\n",
       " (17, 18),\n",
       " (19, 28),\n",
       " (29, 33),\n",
       " (34, 42),\n",
       " (43, 50),\n",
       " (51, 53),\n",
       " (54, 58),\n",
       " (58, 59),\n",
       " (60, 66),\n",
       " (67, 70),\n",
       " (71, 81),\n",
       " (82, 88),\n",
       " (89, 91),\n",
       " (92, 96),\n",
       " (96, 97),\n",
       " (98, 105),\n",
       " (106, 109),\n",
       " (110, 113),\n",
       " (113, 114)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "list(twt().span_tokenize(try_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b36d70-98a5-4e23-bd0e-14cd564eb434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
